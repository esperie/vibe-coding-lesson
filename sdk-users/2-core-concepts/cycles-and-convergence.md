# âš ï¸ CYCLIC WORKFLOWS - PLANNED FEATURE

> **STATUS**: This feature is planned but NOT YET IMPLEMENTED in the current SDK version (v0.9.31).
>
> The cycle convergence functionality described in this document represents planned capabilities.
> While some internal cycle detection and handling exists, the CycleBuilder API and full cyclic workflow
> support is under active development.
>
> **Current Alternatives**:
> - Recursive workflows with manual iteration control
> - External Python loops around workflow execution
> - State machines with conditional routing (SwitchNode)
> - Sequential workflow executions with state persistence

---

# Cycles and Convergence (PLANNED)

**Future Capability**: Kailash SDK will provide robust support for cyclic workflows with reliable convergence, natural termination, and optimal performance.

## ðŸŽ¯ Planned Key Features

- **Deterministic Execution**: Same inputs will always produce same outputs
- **Natural Termination**: Cycles will terminate when SwitchNode conditions change
- **Optimal Performance**: Each node will execute exactly once per iteration
- **Parameter Propagation**: Data will correctly flow between iterations
- **Hierarchical Switch Support**: Multi-level dependent switch execution

## ðŸš€ Quick Start

### Basic Cycle Pattern

```python
from kailash.workflow.builder import WorkflowBuilder
from kailash.runtime.local import LocalRuntime

# Build workflow
workflow = WorkflowBuilder()

# Add nodes
workflow.add_node("PythonCodeNode", "source", {
    "code": "result = {'count': 0, 'continue': True}"
})

workflow.add_node("PythonCodeNode", "processor", {
    "code": """
input_data = parameters if isinstance(parameters, dict) else {}
count = input_data.get('count', 0) + 1
continue_processing = count < 5

result = {
    'count': count,
    'continue': continue_processing,
    'message': f'Iteration {count}'
}
"""
})

workflow.add_node("SwitchNode", "terminator", {
    "condition_field": "continue",
    "operator": "==",
    "value": True
})

# Connect nodes
workflow.add_connection("source", "result", "processor", "parameters")
workflow.add_connection("processor", "result", "terminator", "input_data")

# Create cycle
built_workflow = workflow.build()
cycle = built_workflow.create_cycle("counter_cycle")
cycle.connect("terminator", "processor", mapping={"true_output": "parameters"})
cycle.max_iterations(10)  # Safety limit
cycle.build()

# Execute
runtime = LocalRuntime()
results, run_id = runtime.execute(built_workflow)

print(f"Final count: {results['processor']['result']['count']}")
# Output: Final count: 5 (terminated naturally when continue=False)
```

## ðŸ”§ Advanced Patterns

### Hierarchical Switch Cycles

```python
# Complex cycle with multiple switches
workflow = WorkflowBuilder()

workflow.add_node("PythonCodeNode", "data_source", {
    "code": "result = {'value_a': 10, 'value_b': 20, 'iteration': 0}"
})

workflow.add_node("SwitchNode", "primary_switch", {
    "condition_field": "value_a",
    "operator": "<",
    "value": 50
})

workflow.add_node("SwitchNode", "secondary_switch", {
    "condition_field": "value_b",
    "operator": "<",
    "value": 35
})

workflow.add_node("PythonCodeNode", "dual_processor", {
    "code": """
input_data = parameters if isinstance(parameters, dict) else {}
iteration = input_data.get('iteration', 0) + 1
value_a = input_data.get('value_a', 10) + 5  # Increment by 5
value_b = input_data.get('value_b', 20) + 2  # Increment by 2

result = {
    'value_a': value_a,
    'value_b': value_b,
    'iteration': iteration
}
"""
})

# Build hierarchical connections
workflow.add_connection("data_source", "result", "primary_switch", "input_data")
workflow.add_connection("primary_switch", "true_output", "secondary_switch", "input_data")
workflow.add_connection("secondary_switch", "true_output", "dual_processor", "parameters")

# Create cycle with hierarchical switches
built_workflow = workflow.build()
cycle = built_workflow.create_cycle("hierarchical_cycle")
cycle.connect("dual_processor", "primary_switch", mapping={"result": "input_data"})
cycle.max_iterations(15)
cycle.build()

runtime = LocalRuntime()
results, run_id = runtime.execute(built_workflow)

# Terminates naturally when value_a >= 50 (iteration 8+)
print(f"Final: value_a={results['dual_processor']['result']['value_a']}, "
      f"iteration={results['dual_processor']['result']['iteration']}")
```

### Accumulative Data Processing

```python
# Cycle that builds on previous results
workflow = WorkflowBuilder()

workflow.add_node("PythonCodeNode", "seed", {
    "code": "result = {'accumulated': [], 'count': 0}"
})

workflow.add_node("PythonCodeNode", "accumulator", {
    "code": """
input_data = parameters if isinstance(parameters, dict) else {}
accumulated = input_data.get('accumulated', [])
count = input_data.get('count', 0)

# Build on previous results
new_count = count + 1
new_data = f"step_{new_count}"
new_accumulated = accumulated + [new_data]

result = {
    'accumulated': new_accumulated,
    'count': new_count,
    'continue': new_count < 3
}
"""
})

workflow.add_node("SwitchNode", "continue_check", {
    "condition_field": "continue",
    "operator": "==",
    "value": True
})

# Connect and cycle
workflow.add_connection("seed", "result", "accumulator", "parameters")
workflow.add_connection("accumulator", "result", "continue_check", "input_data")

built_workflow = workflow.build()
cycle = built_workflow.create_cycle("accumulation_cycle")
cycle.connect("continue_check", "accumulator", mapping={"true_output": "parameters"})
cycle.max_iterations(5)
cycle.build()

runtime = LocalRuntime()
results, run_id = runtime.execute(built_workflow)

print(f"Accumulated: {results['accumulator']['result']['accumulated']}")
# Output: Accumulated: ['step_1', 'step_2', 'step_3']
```

## ðŸ”„ Cycle Execution Phases

### Phase 1: Initialization
- Workflow graph analysis and cycle detection
- Execution plan generation with cycle groups
- Safety configuration (max_iterations, timeout, memory_limit)

### Phase 2: Iterative Execution
- **Deterministic execution**: Each node executes exactly once per iteration
- **Parameter propagation**:
  - Cycle edges use previous iteration results (feedback)
  - Non-cycle edges use current iteration results (fresh data)
  - Fallback to cached state for DAG compatibility

### Phase 3: Natural Termination
- **Condition monitoring**: SwitchNode outputs checked each iteration
- **Termination detection**: When condition changes (trueâ†’false or falseâ†’true)
- **Graceful exit**: Execution continues to downstream nodes outside cycle

## âš¡ Performance Characteristics

### Optimizations
- **No Double Execution**: Each node runs exactly once per iteration (50% performance improvement)
- **Smart Parameter Resolution**: Efficient data access with proper caching
- **Memory Efficient**: Minimal state tracking between iterations

### Benchmarks
- **Small cycles** (2-3 nodes): < 1ms overhead per iteration
- **Medium cycles** (5-10 nodes): < 5ms overhead per iteration
- **Large cycles** (20+ nodes): Linear scaling with node count
- **Parameter propagation**: O(1) access time with intelligent caching

## ðŸ›¡ï¸ Safety Features

### Built-in Protections
```python
# Safety configuration
cycle = built_workflow.create_cycle("safe_cycle")
cycle.max_iterations(100)        # Maximum iteration limit
cycle.timeout(30)                # 30 second timeout
cycle.memory_limit(500)          # Memory usage limit in MB
cycle.build()
```

### Convergence Detection
```python
# Custom convergence check
def convergence_check(iteration_results, previous_results):
    """Return True when cycle should terminate."""
    if not previous_results:
        return False

    current_value = iteration_results.get('processor', {}).get('result', {}).get('value', 0)
    previous_value = previous_results.get('processor', {}).get('result', {}).get('value', 0)

    # Terminate when value stops changing
    return abs(current_value - previous_value) < 0.001

cycle.convergence_check(convergence_check)
```

## ðŸ” Debugging and Monitoring

### Execution Logging
```python
import logging
logging.basicConfig(level=logging.DEBUG)

# Enable cycle execution logging
logger = logging.getLogger("kailash.workflow.cyclic_runner")
logger.setLevel(logging.DEBUG)

runtime = LocalRuntime()
results, run_id = runtime.execute(built_workflow)
```

### Monitoring Integration
```python
from kailash.tracking.manager import TaskManager

# Track cycle execution with detailed metrics
task_manager = TaskManager()
runtime = LocalRuntime(task_manager=task_manager)
results, run_id = runtime.execute(built_workflow)

# Access execution metrics
tasks = task_manager.get_tasks_by_run(run_id)
for task in tasks:
    if "cycle" in task.node_id:
        print(f"Cycle {task.node_id}: {task.duration}ms")
```

## ðŸš¨ Common Patterns and Solutions

### Pattern 1: Counter with Natural Termination
```python
# âœ… Good: Natural termination based on counter
workflow.add_node("PythonCodeNode", "counter", {
    "code": """
count = parameters.get('count', 0) + 1
result = {
    'count': count,
    'should_continue': count < 10  # Natural termination condition
}
"""
})

workflow.add_node("SwitchNode", "terminator", {
    "condition_field": "should_continue",
    "operator": "==",
    "value": True
})
```

### Pattern 2: Data Accumulation
```python
# âœ… Good: Building on previous iteration results
workflow.add_node("PythonCodeNode", "accumulator", {
    "code": """
# Access previous results properly
previous_data = parameters.get('data', [])
new_item = f"item_{len(previous_data) + 1}"

result = {
    'data': previous_data + [new_item],
    'continue': len(previous_data) < 5
}
"""
})
```

### Pattern 3: Conditional Processing Chain
```python
# âœ… Good: Multiple conditions with proper hierarchy
workflow.add_node("SwitchNode", "primary_condition", {
    "condition_field": "status",
    "operator": "==",
    "value": "active"
})

workflow.add_node("SwitchNode", "secondary_condition", {
    "condition_field": "score",
    "operator": ">",
    "value": 75
})

# Chain conditions: primary â†’ secondary â†’ processor
workflow.add_connection("primary_condition", "true_output", "secondary_condition", "input_data")
workflow.add_connection("secondary_condition", "true_output", "processor", "parameters")
```

## ðŸ“Š Production Validation

### Test Results (v0.9.0+)
- âœ… **Deterministic execution**: 100% consistent results across multiple runs
- âœ… **Performance**: 50% improvement with elimination of double execution
- âœ… **Natural termination**: Cycles terminate correctly in 100% of test cases
- âœ… **Parameter propagation**: Data flows correctly between iterations
- âœ… **Hierarchical switches**: Complex switch dependencies work reliably

### Validation Suite
Run comprehensive cycle tests:
```bash
# Run cycle convergence validation
python test_todo128_validation.py

# Run integration tests
pytest tests/integration/runtime/test_hierarchical_switch_cyclic.py -v

# Run performance benchmarks
pytest tests/e2e/workflows/ -k "cycle" -v
```

## ðŸŽ‰ Success Stories

**Before TODO-128 fixes**:
- Non-deterministic results (same test, different outputs)
- Double node execution (2x performance overhead)
- Cycles running to max_iterations instead of natural termination

**After TODO-128 fixes**:
- âœ… 100% deterministic execution
- âœ… 50% performance improvement
- âœ… Natural termination when conditions change
- âœ… Originally failing tests now PASS consistently

**Production Impact**: Core cycle convergence functionality is now fully reliable for production workflows with complex conditional logic and iterative processing requirements.

---

## ðŸ“š Related Guides

- **[Workflow Patterns](workflows/)** - Complete workflow examples
- **[SwitchNode Guide](nodes/switch-node-guide.md)** - Conditional routing patterns
- **[Performance Guide](../3-development/performance-optimization.md)** - Optimization techniques
- **[Troubleshooting](../3-development/05-troubleshooting.md)** - Common cycle issues and solutions

**Created**: 2025-01-29
**Status**: Production Ready (v0.9.0+)
